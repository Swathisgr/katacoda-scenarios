#!/usr/bin/env bdwb
################################################################################
#                                                                              #
#  Sample workbench instructions for building a BlueData catalog entry.        #
#                                                                              #
################################################################################
#
# YOUR_ORGANIZATION_NAME must be replaced with a valid organization name. Please
# refer to 'help builder organization' for details.
#
builder organization --name BlueData

## Begin a new catalog entry
catalog new --distroid spark231mjuphub7x --name "Spark 2.3.1 notebooks & Jupyterhub with centos7x"                           \
            --desc "Jupyterhub on Spark 2.3.1 multirole with Notebooks, Jupyterhub and gateway node"			\
            --categories Spark --catalogapi 3 --version 2.1

## Define all node roles for the virtual cluster.
role add controller 0+
role add worker 0+
role add jupyter 0+
role add jupyterhub 0+
role add rstudio 0+
role add gateway 0+

## Define all services that are available in the virtual cluster.

service add --srvcid spark --name "Spark master" --scheme "http" --port 8080   \
            --path "/" --display --onroles controller

service add --srvcid spark_master --name "Spark master" --scheme "spark"       \
        --port 7077 --export_as "spark"    \
	    --sysctl spark-master \
	    --onroles controller

service add --srvcid spark_worker --name "Spark worker" --scheme "http"        \
        --port 8081 --path "/" --display   \
	    --sysctl spark-slave               \
	    --onroles controller worker 

service add --srvcid rstudio-server --name "Rstudio server" --scheme "http" --port 8787   \
        --path "/" --display  \
	    --sysctl rstudioserver             \
	    --onroles rstudio

service add --srvcid shiny-server --name "Shiny server" --scheme "http" --port 3838   \
        --path "/" --display   \
	    --sysctl shinyserver   \
	    --onroles rstudio

service add --srvcid jupyterhub --name "Jupyterhub" --scheme "http" --port 8000   \
        --path "/" --display    \
	    --sysctl jupyterhub \
	    --onroles jupyterhub

service add --srvcid jupyter-notebook --name "Jupyter Notebook" --scheme "http" --port 8888   \
	    --path "/" --display  \
	    --sysctl jupyter-server  \
	    --onroles jupyter

## Appconfiguration autogenenration.
appconfig autogen --new --configapi 7

appconfig autogen --pkgfile start_rstudioserver.sh  --destdir /usr/lib/rstudio-server/conf/ \
		  --pkgfile start_shinyserver.sh   --destdir /opt/shiny-server/conf/ \
		  --pkgfile jupyter_notebook_config.py --destdir /root/.jupyter/ \
	          --pkgfile jupyterhub_config.py --destdir /etc/jupyterhub \
		  --pkgfile spark-defaults.conf spark-env.sh --destdir /usr/lib/spark/spark-2.3.1-bin-hadoop2.7/conf \
		  --pkgfile rstudio --destdir /etc/pam.d/  \
		  --pkgfile pam_mkhomedir.sh --destdir /usr/bin/ \
		  --pkgfile rstudioserver shinyserver spark-master spark-slave jupyter-server start_jupyter.sh jupyterhub start_jupyterhub.sh --destdir /etc/init.d/   

appconfig autogen --pkgfile core-site.xml --dest /usr/lib/spark/spark-2.3.1-bin-hadoop2.7/conf/core-site.xml \
		  --pkgfile hadoop --dest /usr/bin/hadoop  \
		  --pkgfile jupyter --dest /etc/sudoers.d/  \
		  --pkgfile appjob --dest /opt/bluedata/vagent/guestconfig/appconfig/appjob  \
		  --pkgfile p_kernel.json --dest /usr/local/share/jupyter/kernels/apache_toree_pyspark/kernel.json  \
		  --pkgfile sq_kernel.json --dest /usr/local/share/jupyter/kernels/apache_toree_sql/kernel.json 

#Replace

appconfig autogen --execute total_vcores.sh --onroles controller worker 

appconfig autogen --replace /usr/local/share/jupyter/kernels/apache_toree_pyspark/kernel.json  \
                  --pattern @@@@SPARK_MASTER@@@@ --macro "GET_SERVICE_URL spark_master controller"

appconfig autogen --replace /usr/local/share/jupyter/kernels/apache_toree_sql/kernel.json  \
                  --pattern @@@@SPARK_MASTER@@@@ --macro "GET_SERVICE_URL spark_master controller"

appconfig autogen --replace /root/.jupyter/jupyter_notebook_config.py	 \
                  --pattern @@@@IP@@@@ --macro "GET_NODE_FQDN"

appconfig autogen --replace /usr/lib/spark/spark-2.3.1-bin-hadoop2.7/conf/spark-defaults.conf \
                  --pattern @@@@SPARK_MASTER@@@@ --macro "GET_SERVICE_URL spark_master controller" \
                  --pattern @@@@SPARK_MAX_CORES@@@@ --macro "GET_TOTAL_VCORES" 

appconfig autogen --replace /usr/lib/spark/spark-2.3.1-bin-hadoop2.7/conf/spark-env.sh        \
                  --pattern @@@@MASTER_HOST@@@@ --macro "GET_FQDN_LIST controller" \
                  --pattern @@@@MEMORY@@@@ --macro "echo $(GET_TOTAL_VMEMORY_MB)m" \
                  --pattern @@@@CORES@@@@ --macro "GET_TOTAL_VCORES"

appconfig autogen --replace /etc/init.d/spark-slave --pattern @@@@FQDN@@@@ --macro GET_NODE_FQDN \
		  --pattern @@@@SPARK_HOME@@@@ --macro "echo /usr/lib/spark/spark-2.3.1-bin-hadoop2.7" \
		  --pattern @@@@SPARK_MASTER@@@@  --macro "GET_SERVICE_URL spark_master controller"

appconfig autogen --replace /etc/init.d/spark-master --pattern @@@@FQDN@@@@ --macro GET_FQDN_LIST controller \
		  --pattern @@@@SPARK_HOME@@@@ --macro "echo /usr/lib/spark/spark-2.3.1-bin-hadoop2.7" \
		  --pattern @@@@SPARK_HOME@@@@ --macro "echo /usr/lib/spark/spark-2.3.1-bin-hadoop2.7"

appconfig autogen --replace /usr/lib/spark/spark-2.3.1-bin-hadoop2.7/conf/core-site.xml \ 
		  --pattern @@@@AWS_ACCESS_KEY@@@@ --macro TENANT_INFO s3_access_key  \
		  --pattern @@@@AWS_SECRET_KEY@@@@ --macro TENANT_INFO s3_secret_key

# node is expected to run the service.

appconfig autogen --generate
appconfig package

## Specify a logo file for the
logo file --filepath Logo_spark.png

################################################################################
#                        Catalog package for CentOS                            #
################################################################################
image build --basedir image/centos --image-repotag bluedata/spark231:1.0
image package --image-repotag bluedata/spark231:1.0 --os centos7
catalog save --filepath staging/Spark231-Jupyterhub-7x.json --force
sources package
catalog package

################################################################################
#                        Catalog package for RHEL                              #
#                                                                              #
# Set RHEL_USERNAME and RHEL_PASSWORD environment variables before invoking    #
# the workbench. The Dockerfile uses subscription-manager to subscribe before  #
# installing any packages. The subscription is cleaned up at the end of the    #
# build process in the same Dockerfile.                                        #
################################################################################
#image build --basedir image/rhel --imgversion 1.0 --os rhel
#catalog save --filepath staging/Jupyterhub-spark221-rhel.json --force
#sources package
#catalog package --os=rhel
